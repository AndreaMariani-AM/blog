[{"content":"My Guilty Pleasure This post\u0026rsquo;s gonna be a little bit different. I wanted to share one of my biggest guilty pleasures, and that is watching videos from 3Blue1Brown, specifically nerdy videos about numbers and maths. I have never been a huge fan of these topics, to be honest, but I\u0026rsquo;ve found out that watching his videos is kinda intriguing, and ofc very informative.\nFor those of you that ignore who 3Blue1Brown is, his names’ Grant Sanderson, and has a youtube channel where it combines math and entertainment. He makes a wide range of videos covering lots and lots of topics always trying to make abstract ideas more graspable. The video I want to focus on today is a particular one, and actually, it\u0026rsquo;s an appendix video of a longer one that tried to explain what cryptocurrencies are and how they work. What I want to do with this post is to summarize and report the examples used in the video to explain a single number. This is the video I\u0026rsquo;m talking about, and tries to answer the question \u0026ldquo;Yo ok, cryptos are cool, but how secure they are?\nAnd the answer to that is $2^256$. The Video Let\u0026rsquo;s give a bit of context before we dive into the analysis of that video, what\u0026rsquo;s $2^256$ ?. To make this as straightforward as possible, when doing transactions with cryptocurrencies, be them done virtually, a digital signature is required to authorize transactions. This digital signature changes every time and makes whatever we\u0026rsquo;re doing legit. But, given that digital signatures are composed of 256 bits characters that are combinations of 0s and 1s, shouldn\u0026rsquo;t we be able to somehow forge a signature?\nAnd this question introduces the whole post, which is, How Stupidly Bad We Are at Understanding Big Numbers. Since digital signatures are generated at random, a potential crazy-cyber-son-of-a-gun thief has no better alternative than randomly generate combinations of 0s and 1s 256 bits long, to forge our signature. The guessed combination should be run through a hash function that\u0026rsquo;ll yield a TRUE or FALSE statement. This is the process in a nutshell, you generate a combination of 0s and 1s, you test it with a hash function, the result will tell you if you\u0026rsquo;ve found the correct digital signature associated with that transaction.\n\u0026ldquo;Yeah ok, but how many combinations are there?\u0026rdquo;\nWell, on average, it\u0026rsquo;d require to generate and test $2^256$ combinations, and we can\u0026rsquo;t even try to understand how big of a number that is. It\u0026rsquo;s insane how our brains can\u0026rsquo;t even process or imagine something that big that we immediately lose grip on reality. It\u0026rsquo;s so far away from what we deal daily that\u0026rsquo;s hard to appreciate how stupid that number is.\nThe Number Itself \u0026ldquo;Yeah cool ok, big big number, ok. Now, give me something to work with so I can understand it a bit better\u0026rdquo;.\nWe\u0026rsquo;ll exploit Grant\u0026rsquo;s video to put this in perspective and use numbers that we\u0026rsquo;re more familiar with. Now, I know I\u0026rsquo;ve just said \u0026ldquo;more familiar\u0026rdquo; but even with numbers like Billions, it\u0026rsquo;s still incredibly hard to grasp them and the orders of magnitude are blurry. Just for reference, 4 million seconds correspond to 46 days, 4 billion, on the other hand, are 127 years. First time I\u0026rsquo;ve heard this, I couldn\u0026rsquo;t understand how my deductions were so far off from the truth, I couldn\u0026rsquo;t understand why the gap between 4 million and 4 billion was that huge.\nSo, how secure is $2^256$?\nLet\u0026rsquo;s start by breaking it down to a bit, $2^256$ is $2^32$ multiplied by itself 8 times. $2^32$ is slightly easier to imagine since it\u0026rsquo;s somewhere around 4 billion, so now we just need to appreciate what multiplying 4 billion by itself 8 times means. Let\u0026rsquo;s work with that and do a little thought experiment to give meaning to those numbers.\n  Imagine you\u0026rsquo;re the best hacker in the world, your super-specialized hash function is in place, your GPU is firing up and ready to parallelize the hash function.\nA really good PC can probably run around 1 billion hash functions per second. But you the best one, and you stuff your PC somehow with more GPUs, and boom, you\u0026rsquo;re up to 4 billion hashes per second (aka, 4 billion guesses per second). Let\u0026rsquo;s call it Big Ass Computer.\n  Now pretend you\u0026rsquo;re a millionaire and you just bought Google. There are no public numbers regarding how many servers does Google have but rumors have it in the millions ballpark.\nSince you\u0026rsquo;re a millionaire, you replace all of the existing crappy Google\u0026rsquo;s computers that are nowhere near your Big Ass Computer in terms of GPUs power, with a copy of your guessing beast.\nAnd while you are at it, why not creating a thousand copies of this Google on steroid? Now, you up to 4 billion Big Ass Computers.\n  Being so generous, you just gave to a bit more than half of the population on heart, 4 billion people, their personal super performing Google.\n  What if our galaxy had 4 billion copies of planet earth where 4 billion people have their own Google with 4 billion Big Ass Computers that can each generate and test 4 billion signatures every second?\n  And what if, there were 4 billion copies of our galaxy? I know you\u0026rsquo;re losing it, hang in there.\n  4 billion seconds? that\u0026rsquo;s about 127 years\n  And 4 billion times 127 is 507 billion years, which is roughly 37 times the age of our universe.\n  So where I\u0026rsquo;m going with this? Even if you had 4 billion galaxies, each containing 4 billion copies of planet earth where each planet has 4 billion people equipped with their own personal Google comprising of 4 billion Big Ass Computers, running each 4 billion guesses every second, and you\u0026rsquo;d go on and on guessing for 37 times the age of the whole universe, you\u0026rsquo;d still only have a 1 in a 4 billion chance of finding the right guess and successfully forge a digital signature. That\u0026rsquo;s how big that number is.\n  Conclusion \u0026ldquo;Ok, cool. That\u0026rsquo;s a big number, but who cares?\u0026rdquo;\nI don\u0026rsquo;t know, I think I do. First of all, I\u0026rsquo;m kinda sad for our imaginary hacker. I mean, all of the odds are against this poor guy. But mostly, I\u0026rsquo;ve found out that these weird numbers and probabilities, fascinate me. How two numbers that we\u0026rsquo;re familiar with, 2 and 256, can really make us wander through imaginary galaxies and computers, is incredible. With that being said, I\u0026rsquo;m waiting for the next 3Blue1Brown video, to blow my mind.\n","date":"2021-04-19T00:00:00Z","image":"https://genstud.netlify.app/p/can-we-even-appreciate-how-big-is-2-to-the-power-256/pexels-dids-3530102_hu3d03a01dcc18bc5be0e67db3d8d209a6_2058949_120x120_fill_q75_box_smart1.jpg","permalink":"https://genstud.netlify.app/p/can-we-even-appreciate-how-big-is-2-to-the-power-256/","title":"Can We Even Appreciate How Big is 2 to the Power 256?"},{"content":"  We’re looking at nonlinear and noncontinuous models for regression in this post.\nLet’s introduce CART and what that is. CART ’s an umbrella term that stands for Classification And Regression Tree and therefore comprises Classification Trees, used in another type of machine learning models (i.e. Classification), and Regression Tree used in Regression models.\nWe’ll talk about the latter in this post. A Regression Tree is one of the two Decision Trees used in data mining and machine learning and it’s used when the outcome/value we’re predicting is a continuous variable(not a “class” like the Classification one).\nNow, we’re gonna explain how this algorithm works in the simplest way possible, aka the only way my brain can handle it.\nIt starts by creating a Root Node which is basically the starting point from a given data set. The Root Node is then split into subsets based on some features and the process is repeated again and again (technical term of “again and again” is Recursive Partitioning) until some conditions are met.\nEvery node leads to a leaf (Tree huh) which is labeled either with a class or a probability distribution, and that leaf leads you to another node.\nThe process stops when it can no longer add value to the newly created leaf and the number of splits and their values are determined by the algorithm (look up “Information Entropy” should you wish to do so).\nAnd that’s pretty much it, it’s like walking down a street and turn left and right based on some inputs. At every node a “question” will be prompted, something along the line with “is x less this”, “is x this or that” and following down that path you’ll end up in a Terminal Leaf which will yield the output, be it a number or a class.\nTalking more about data and numbers, given a 2 dimensional dataset with two independent variables \\(x_1\\) and \\(x_2\\), the dependent variable \\(y\\) will be predicted based on \\(x_1\\) and \\(x_2\\) values.\nStarting from the Root Node, \\(y\\) will go through the Decision Tree until it lands in a Terminal Leaf where its value will be computed based on the average of all the other observations that are in that leaf.\nUsing a single Regression Tree ain’t the best idea, its bigger buzz kill is that ain’t a robust method, as even a small change in the data used to train it, can result in a huge change in the final prediction.\nBut here is where Ensemble Learning comes in clutch. Random Forest Regression, which is a version of Ensemble Learning, allows us to bypass the internal variability of a Regression Tree Model and use an on steroid version of that model.\nLastly, I think its bigger advantage comes from using a White Box Model that’s easily explainable by Boolean Logic. White and Black box models are super interesting btw, so maybe we’ll talk more about them.\nNow let’s see some code.\n# Importing the dataset dataset = read.csv(\u0026#39;Position_Salaries.csv\u0026#39;) dataset = dataset[2:3] Here we’re just loading in the dataset and selecting our independent variable (Level, as in the Level that corresponds to a position in the company x) and dependent variable (Salary that’s associated with that Level).\nThere’s no need for feature scaling ’cause the model is built on conditions on the independent variable and doesn’t rely on Euclidian Distances.\n# Building the model regressor \u0026lt;- rpart(formula = Salary ~ ., data = dataset) # Predicting a Level that\u0026#39;s not in the dataset y_pred = predict(regressor, data.frame(Level = 6.5)) This is how we’re building the model, there are lots and lots of parameters that can be tweaked, but for now, we’re just sticking to the basics.\nOne thing’s extremely important here and that’s the parameter control, explicitly left out here.\nThat prediction is off, and when plotted we can see how no sense it is.\nggplot() + geom_point(aes(x = dataset$Level, y = dataset$Salary), colour = \u0026#39;red\u0026#39;) + geom_line(aes(x = dataset$Level, y = predict(regressor, newdata = dataset)), colour = \u0026#39;blue\u0026#39;) + ggtitle(\u0026#39;Decison Tree Regression\u0026#39;) + xlab(\u0026#39;Level\u0026#39;) + ylab(\u0026#39;Salary\u0026#39;)  Figure 1: Decision Tree Regression, No Control Parameter  Something’s wrong here. The model here is just a straight line, not what we would expect as a Decision Tree; though in reality, that’s our Decision Tree, just an awful one.\nWe’re missing the parameter that defines splits and conditions on the independent variable. To set this, we can use the parameter control as an argument and call the rpart.control() function from rpart package.\n# adding the \u0026quot;control\u0026quot; parameter regressor \u0026lt;- rpart(formula = Salary ~ ., data = dataset, control = rpart.control(minsplit = 1)) # Plotting again ggplot() + geom_point(aes(x = dataset$Level, y = dataset$Salary), colour = \u0026#39;red\u0026#39;) + geom_line(aes(x = dataset$Level, y = predict(regressor, newdata = dataset)), colour = \u0026#39;blue\u0026#39;) + ggtitle(\u0026#39;Decison Tree Regression\u0026#39;) + xlab(\u0026#39;Level\u0026#39;) + ylab(\u0026#39;Salary\u0026#39;)  Figure 2: Decison Tree Regression, Low Resolution  This plot still’s off, isn’t it? We can see the regressor is kinda linear kinda not, but also kinda Polynomial even though the regressor is broken down into segments. So why is that? Well, the problem here lies in the noncontinuity of the model, and this visualization can’t fully represent our model.\nTo make the visualization better, a higher resolution and a smoother curve are needed.\nx_grid = seq(min(dataset$Level), max(dataset$Level), 0.01) ggplot() + geom_point(aes(x = dataset$Level, y = dataset$Salary), colour = \u0026#39;red\u0026#39;) + geom_line(aes(x = x_grid, y = predict(regressor, newdata = data.frame(Level = x_grid))), colour = \u0026#39;blue\u0026#39;) + ggtitle(\u0026#39;Decison Tree Regression\u0026#39;) + xlab(\u0026#39;Level\u0026#39;) + ylab(\u0026#39;Salary\u0026#39;)  Figure 3: Decison Tree Regression, High Resolution  Better huh? looks more like a decent one. What we’ve done here is create an x_grid for the \\(x\\) argument of the geom_line aesthetic, so for every Level in the dataset, with a 0.01 resolution, a \\(y\\) value is predicted through the model.\nfit \u0026lt;- regressor \u0026lt;- rpart(formula = Salary ~ ., data = dataset, control = rpart.control(minsplit = 1)) rpart.plot(fit, main = \u0026quot;Decision Tree\u0026quot;)  Figure 4: Decision Tree  Just in case you need to plot the Tree and see what the splits were.\nCarbon Figure\n ","date":"2021-04-12T00:00:00Z","image":"https://genstud.netlify.app/p/decision-tree-regression/pawel-czerwinski-U1vCHP__C44-unsplash_hue6b9207563d19f3097e010c4986fa075_1844977_120x120_fill_q75_box_smart1.jpg","permalink":"https://genstud.netlify.app/p/decision-tree-regression/","title":"Decision Tree Regression"},{"content":"  In this post, we’ll talk about Simple Linear Regression and Polynomial Linear Regression. These two represent the backbone for linear and continuous regressions.\nThere’s also Multiple Linear Regression but it requires a small chat about model building, so it’ll be the content for a future post.\nFirst things first, let’s dive deep into what is Regression. Regression can be defined as a set of statistical processes that are trying to estimate the relationship between a dependent variable and one or more independent variables, creating a model that best fits the data.\nWe’ll discuss more about what “best fit” means in a moment.\nToday, it’s often used as a way to predict the value of a dependent variable, given an independent variable.\nWe can roughly divide Regression into Linear and Non-Linear Regression, and today we’ll focus on the linear (and continuous) ones.\n\\[ y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\]\nThis is the model that represents a Simple Linear Regression and it’s a straight line that fits the data, where \\(y\\) is the dependent variable, \\(x\\) is the independent variable, and \\(\\beta_0\\) and \\(\\beta_1\\) are regression coefficients that measure the relationship between \\(y\\) and \\(x\\) .\n\\(\\beta_0\\) is a constant and represents where the line crosses the vertical axis, while \\(\\beta_1\\) represents the slope of the line. \\(\\epsilon_i\\) is a disturbance error or error variable that helps model the relationship between \\(y\\) and \\(x\\) adding an unobservable “noise”.\nWhen we talk about model that best fits the data we’re referring to the way the algorithm for Linear Regression builds the model. In other non-scientific words, given data points in a scatterplot, what is the line that best represents the data or best adapts to the data and how can we obtain it?\nThere are all different kinds of methods for data fitting, and for Linear Regression the Least Squares method represents the standard approach.\nBasically, it draws an \\(n\\) number of lines that fits the data (aka goes through them), and for each line, it calculates the sum of squared residuals where the residual is the difference between the observed values \\(y\\) and the fitted value \\(\\hat{y}\\) that’s provided by the model.\n\\[ \\sum\\limits_{i=1}^{n}(y_i – \\hat{y}_1)^2 = min \\]\nEvery time, the algorithm tries to minimize this sum and it stops when it’s no longer able to minimize it and that’s the model or the line that best fits the data.\nBefore building a Linear Regression Model, some assumptions need checking.\nLinearity Homoscedasticity Multivariate Normality Independence of Errors Lack of Multicollinearity  We won’t talk about this ’cause there’s no time and I’m getting tired of typing on the keyboard, and plus we still gotta see how to build a model in R. And oh gosh, the Polynomial Regression, we need to check out that one too, almost forgot. Gotta move here, chop chop mf.\nLet’s see how we can build a simple Linear Regression Model. Dataset used are very small and just for article’s sake.\n# Trying to predict the Salary based on Years of Experience regressor = lm(formula = Salary ~ YearsExperience, data = training_set) You can call summary(regressor) to look at coefficients.\nNow let’s visualize the Regressor, where the blue line represents our regressor and the red points represent datapoints.\nplot_training \u0026lt;- ggplot() + geom_point(aes(x = training_set$YearsExperience, y = training_set$Salary), colour = \u0026#39;red\u0026#39;) + geom_line(aes(x = training_set$YearsExperience, y = predict(regressor, newdata = training_set)), colour = \u0026#39;blue\u0026#39;) + ggtitle(\u0026#39;Salary vs Experience (Training set)\u0026#39;) + xlab(\u0026#39;Years of experience\u0026#39;) + ylab(\u0026#39;Salary\u0026#39;) plot_test \u0026lt;- ggplot() + geom_point(aes(x = test_set$YearsExperience, y = test_set$Salary), colour = \u0026#39;red\u0026#39;) + geom_line(aes(x = training_set$YearsExperience, y = predict(regressor, newdata = training_set)), colour = \u0026#39;blue\u0026#39;) + ggtitle(\u0026#39;Salary vs Experience (Test set)\u0026#39;) + xlab(\u0026#39;Years of experience\u0026#39;) + ylab(\u0026#39;Salary\u0026#39;) Ok now let’s talk about Polynomial Linear Regression\n\\[ y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_1^{2} + \\cdots + \\beta_n x_1^{n} + \\epsilon_i \\]\nLet’s address the elephant in the room, how’s this even linear? Well, Although it fits a nonlinear model to the data, as far as we’re concerned, as a statistical estimation problem it is linear, in the sense that the regression function is linear in the unknown parameters that are estimated from the data. For this reason, it’s also considered a special case of Multiple Linear Regression.\nConfusing much? We’ll circle back to this when talking Multiple Linear Regression.\nTo explain this concept in other words, we can say that a Polynomial Regression Model is composed of one independent variable and additional independent variables that are polynomial terms of the first independent variable.\nLet’s see what it looks like. We’ll build both a Linear Regressor and Polynomial Regressor for the same dataset to compare them and see how they are able to predict values.\n# Simple Linear Regressor Model lin_reg \u0026lt;- lm(formula = Salary ~ Level, data = dataset1) # Polynomial Regressor Model poly_reg \u0026lt;- lm(formula = Salary ~ ., data = dataset2) Now let’s plot them and see how they behaves\nplot_lin \u0026lt;- dataset1 %\u0026gt;% ggplot() + geom_point(mapping = aes(x = Level, y = Salary), color = \u0026quot;red\u0026quot;) + geom_line(mapping = aes(x = Level, y = predict(lin_reg, newdata = dataset1)), color = \u0026quot;blue\u0026quot;) + ggtitle(\u0026quot;linear Regressor\u0026quot;) + xlab(\u0026quot;Level\u0026quot;) + ylab(\u0026quot;Salary\u0026quot;) plot_poly \u0026lt;- dataset2 %\u0026gt;% ggplot() + geom_point(mapping = aes(x = Level, y = Salary), color = \u0026quot;red\u0026quot;) + geom_line(mapping = aes(x = Level, y = predict(poly_reg, newdata = dataset2)), color = \u0026quot;blue\u0026quot;) + ggtitle(\u0026quot;Polynomial Regressor\u0026quot;) + xlab(\u0026quot;Level\u0026quot;) + ylab(\u0026quot;Salary\u0026quot;)  That was it, hope you enjoyed that.\nCarbon\n ","date":"2021-04-05T00:00:00Z","image":"https://genstud.netlify.app/p/simple-linear-regression-and-polynomial-linear-regression/pexels-eberhard-grossgasteiger-2088205_hu3d03a01dcc18bc5be0e67db3d8d209a6_1607845_120x120_fill_q75_box_smart1.jpg","permalink":"https://genstud.netlify.app/p/simple-linear-regression-and-polynomial-linear-regression/","title":"Simple Linear Regression and Polynomial Linear Regression"},{"content":"  This post is based on the Machine Learning A-Z™: Hands-on Python \u0026amp; R In Data Science course on Udemy that i’m taking. It’s a general introduction to the most used and known machine learning algorithms. This is the first chapter and a non-negotiable step not only for machine learning but also for every data analysis.\nThis’ll be a blend of concepts introduced in the course and other personal considerations to make it more complete. There few general steps to take into account\n Understanding which variables are dependent and independent\n Load the data and ponder whether you should impute or remove missing data\n Encode categorical variables miss interpreted by R and re-encode them as binary\n Splitting dataset into training and test set\n Feature Scaling with Standardization or Normalization\n  First of all, let’s import the data set.\ndataset1 Country Age Salary Purchased 1 France 44 72000 No 2 Spain 27 48000 Yes 3 Germany 30 54000 No 4 Spain 38 61000 No 5 Germany 40 NA Yes 6 France 35 58000 Yes 7 Spain NA 52000 No 8 France 48 79000 Yes 9 Germany 50 83000 No 10 France 37 67000 Yes Well, there’s no way to proceed forward with any machine learning model if no dependent and independent variables are identified.\nThere shouldn’t be the need to explain this but what we’re trying to do is to find a relationship between the independent variable and the dependent variable so that we can observe how one can influence the other.\nSometimes it’s pretty clear who’s dependent and who’s independent, but many times it takes a bit of prior knowledge/thinking to correctly identify who’s who.\nAlso, and we’re talking more in-depth about that when we’re seeing regression models, not every independent variable has the same effect on the dependent, and their role needs to be assessed.\nDealing with missing data can be a real pain in the ass. Though it’s a crucial step, too often it’s treated as “yo, delete missing values and go on” and that can ruin your analysis.\nWhether to impute or delete missing values it’s an intricated topic that requires a lot of critical thinking and it’s also highly case dependent.\nThis removes all missing data (NAs).\ndataset1 \u0026lt;- na.omit(dataset1) dataset1 Country Age Salary Purchased 1 France 44 72000 No 2 Spain 27 48000 Yes 3 Germany 30 54000 No 4 Spain 38 61000 No 6 France 35 58000 Yes 8 France 48 79000 Yes 9 Germany 50 83000 No 10 France 37 67000 Yes Let’s say that we’ve decided we’re going with data deletion, there’re a couple of general rules we have to take into account when doing it. Again, it’s highly dependent on your specific case but we can at least agree on something.\nThe amount of data that’s missing should be as low as possible and the whole dataset should have enough observation. Unfortunately, there’re no clear guidelines on what’s the acceptable number of missing data related to the size of the dataset.\n If a whole observation has missing data for every variable, we can take into consideration the removal of the observation (optimal if these missing observations represent a casual subgroup of the whole group).\n If a variable has missing data for every observation, we can take into consideration the removal of the variable (not if the variable is fundamental for our analysis).\n  Both methods should be pondered seriously and both suffer from the risk of inducing a Selection Bias, ’cause we gotta understand the underlying mechanism that’s generated the missing values.\nIf there’s a reason why those values are missing we could completely misdirect the analysis and unconsciously select a group that’s not representative of the whole population.\nIf the missing values are randomly distributed, removing them might cause the loss of too much info and therefore might be safer to impute them.\nmd.pattern(dataset2)  Country Purchased Age Salary 8 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 2 See what’s the pattern of missing data and then impute them\ntempData \u0026lt;- mice(dataset2,m=5,meth=\u0026#39;pmm\u0026#39;,seed=500) completedData \u0026lt;- complete(tempData,1) We can decide to impute data using the mean and mice package.\nIf you decide imputing data is the road you wanna go with, next step it’s to assess how to impute data. For short, imputing means to substitute missing data with new alternatives that’re coherent/plausible, obtained from the dataset or external sources, or both.\nThe idea is to reduce the distortion introduced by missing data to the lowest degree possible. Ofc they have their fair share of disadvantages, ’cause we are considering our imputed data as “true observations”. The underlying idea is to preserve statistical accuracy.\nThere are different ways to impute data, and all have to be considered based on the internal characteristics of the dataset, but we can summarize them as\n Imputing the mean (reduces internal variance)\n Imputing the median\n Hot deck imputation\n Imputation through EM (expectation-maximization)\n  Another useful way to do it is through the function ifelse\ndataset3$Age = ifelse(is.na(dataset3$Age), ave(dataset3$Age, FUN = function(x) mean(x, na.rm = TRUE)), dataset3$Age) dataset3$Salary = ifelse(is.na(dataset3$Salary), ave(dataset3$Salary, FUN = function(x) mean(x, na.rm = TRUE)), dataset3$Salary) Another thing we gotta keep in mind is that R most of the time misinterprets the class of some data and it needs to be fixed.\nTo do so, we need to convert categorical data that’s been interpreted as numeric to factor, and encode strings of characters to numbers to be able to feed them to machine learning algorithms.\ndataset3$Country = factor(dataset3$Country, levels = c(\u0026#39;France\u0026#39;, \u0026#39;Spain\u0026#39;, \u0026#39;Germany\u0026#39;), labels = c(1, 2, 3)) dataset3 Country Age Salary Purchased 1 1 44.00000 72000.00 No 2 2 27.00000 48000.00 Yes 3 3 30.00000 54000.00 No 4 2 38.00000 61000.00 No 5 3 40.00000 63777.78 Yes 6 1 35.00000 58000.00 Yes 7 2 38.77778 52000.00 No 8 1 48.00000 79000.00 Yes 9 3 50.00000 83000.00 No 10 1 37.00000 67000.00 Yes It’s time to split the data into training and test set, and we’re gonna use a package called caTools that allows us to split the samples given a variable.\nWe’re also setting a seed to allow reproducibility of results.\nset.seed(123) split = sample.split(dataset3$Salary, SplitRatio = 0.8) training_set = subset(dataset3, split == TRUE) test_set = subset(dataset3, split == FALSE) Usually, a good split ratio is around 75/80 % allowing for enough data to train the model and enough data to test it.\ntraining_set = scale(training_set) test_set = scale(test_set) Feature scaling is usually done when two variables are not on the same scale and therefore is impossible to predict values. This happens because one variable completely dominates the other.\nSome packages by default apply a scaling factor (through Standardization or Normalization). If feature scaling is needed, this will do the trick.\nThat was it, a quick intro to data preprocessing for machine learning algorithms.\nCarbon Code\n ","date":"2021-03-29T00:00:00Z","image":"https://genstud.netlify.app/p/data-preprocessing-for-machine-learning/pexels-squeeb-creative_hu3d03a01dcc18bc5be0e67db3d8d209a6_5368138_120x120_fill_q75_box_smart1.jpg","permalink":"https://genstud.netlify.app/p/data-preprocessing-for-machine-learning/","title":"Data Preprocessing for Machine Learning "},{"content":"First of all, What\u0026rsquo;s this? This is the program I\u0026rsquo;m enrolled in atm and as of today it\u0026rsquo;s still named \u0026ldquo;Secondary Level Master\u0026rsquo;s Degree\u0026rdquo;. Wait, let\u0026rsquo;s cut the chase right out, the heck is this? For all the non-Italians out there, this needs a bit of an explanation and context. To be completely honest, I\u0026rsquo;m not aware if something similar even exists outside of this country.\nAnyways, Italian\u0026rsquo;s higher education is roughly structured as follows:\n Bachelor\u0026rsquo;s degree (3 years) Master\u0026rsquo;s degree( 2 years) Full-on 5/6 years degrees (like medicine or law, for example)  This is considered as a \u0026ldquo;first-level\u0026rdquo;, or something like that, it\u0026rsquo;s quite foggy though. After this, you can take on one of these paths:\n PhD School of specialization First level Master\u0026rsquo;s degree Second level Master\u0026rsquo;s degree  And here is where it gets tricky \u0026lsquo;cause these degrees have the same exact name as the Master that follows a bachelor. To make this as simple as possible, a first-level Master\u0026rsquo;s degree is a Master that can be attended by bachelor students (undergrads), while a second-level Master\u0026rsquo;s is a Master\u0026rsquo;s that can be attended by Master\u0026rsquo;s students (post-bachelor, grad student).\nI know this is hella confusing but bear with me for a second. The idea underlying this structure is that these first/second-level Masters should bridge the gap between Universities and companies, giving the possibility to students to specialize in a sector and also gain work experience while doing it. Then what are schools of specialization, you might ask. Honestly, I have no clues.\nWhat\u0026rsquo;s the Structure Like How\u0026rsquo;s structured then? Well, it blends courses/modules and independent study spread over two years. It\u0026rsquo;s not as grade-centric as BSc or MSc are, but it does test you. In these two years, you\u0026rsquo;re also expected to do an internship on a subject that\u0026rsquo;s specific to the course, and the ultimate goal, in my case, is to produce a relevant thesis and hopefully a decent publication.\nIt has 5 modules per year for two years plus some conferences and seminars throughout the whole year. Those will be more field-specific and organized within the student\u0026rsquo;s interests.\nModules and Courses As said before, it\u0026rsquo;s divided into 5 modules per year and this\u0026rsquo;s a brief overview of the modules.\nFirst year   Statistics with R. Ofc starts off with an introduction of classical statistical inference (parametric and non-parametric statistics), data visualization in R, and a general introduction to R programming language and RStudio.\n  Regression Models. It comprises linear and logistic regression, linear mixed models, and survival analysis.\n  Resampling Methods. Covers a variety of topics like permutation/bootstrapping and randomization, Monte Carlo simulation, FDR, and empirical p-values.\n  Epidemiology Applied to Genetics. Covers the design and the interpretation of various epidemiological studies.\n  Statistics applied to Genetics. It\u0026rsquo;s all about them Genome-Wide (GW) studies.\n  Second year   Handling and Analysis of Big Data. Cloud and parallel computing, use of databases (SQL and NoSQL), and introduction to Python programming.\n  Bioinformatics Applied to Genetics. I\u0026rsquo;m so waiting for this. Next-gen seq, DNA/RNA-seq, variant calling, and Differential expression analyses.\n  Statistics Applied to Genomics. Lasso and elastic-net regression, dimensionality reduction, path and heredity analysis.\n  POST-GWAS. This also looks quite interesting, ranging from polygenic risk score and cross-validation, Meta-analysis of GWA studies to Mendelian randomization and method of Omics integrations.\n  Machine Learning. This covers supervised/unsupervised/deep learning applied to genomics.\n  What Am I Expecting Out of This Degree? Let\u0026rsquo;s close this off by looking at what\u0026rsquo;s ahead. What will I get out of this program? Well, too early to tell but if I were to guess, I\u0026rsquo;d say that at least I should be a statistical ninja. Hope\u0026rsquo;s that statistical skills will be sound and crystal clear. Also, I do think that statistics involves a lot of critical thinking and pragmatic decision-making, and we don\u0026rsquo;t ever have enough of that, to be honest. Too much no sense interpretation of data happens in many fields, almost as if they were just made on the spot, so we better off without all of that. I can\u0026rsquo;t really tell what will happen during this time, I only know that it\u0026rsquo;ll be fun.\nAnd yeah, someone please give me a job, it\u0026rsquo;s all fun and games until I can no longer pay for my snacks. Boy\u0026rsquo;s hungry, boy gotta eat.\nSee ya.\n","date":"2021-03-22T00:00:00Z","image":"https://genstud.netlify.app/p/genomic-data-science/pexels-madison-inouye-1831234_hu3d03a01dcc18bc5be0e67db3d8d209a6_190291_120x120_fill_q75_box_smart1.jpg","permalink":"https://genstud.netlify.app/p/genomic-data-science/","title":"Genomic Data Science"},{"content":"Although it may seem that I\u0026rsquo;m referring to the current COVID-19 situation, this is actually a post on how I\u0026rsquo;ve decided to ruin my life once and for all, with everything I\u0026rsquo;d like to know by the end of the year. Seems like a heck of a long shot now that I\u0026rsquo;m reading this again, but let this boy dream. Below some stuff I\u0026rsquo;m studying now and something that\u0026rsquo;s planned for the future.\nHit \u0026lsquo;em with the Nested List  Courses I\u0026rsquo;m currently taking  Genomic Data Science R for Data Science Machine Learning A-Z™: Hands-on Python \u0026amp; R In Data Science   Courses I\u0026rsquo;ve planned  Single Cell RNA-seq Data Analysis Data Visualization \u0026amp; Dashboarding with R Statistics for Genomic Data Science Statistical Inference and Modeling for High-throughput Experiments High dimensional Data Analysis Case Studies in Functional Genomics HarvardX Biomedical Data Science Open Online Training SQL for data science    What\u0026rsquo;s cooking? These are courses I\u0026rsquo;ve chosen to start with, two of them are actually thought through and well planned. Third one\u0026rsquo;s just an impulse and probably the least needed at this point (aka the ML one). But what can you do when your brain goes rouge and decides on its own, if not following it?\nGenomic Data Science I won\u0026rsquo;t redirect to the website of the Master\u0026rsquo;s program simply \u0026lsquo;cause it\u0026rsquo;s only in Italian and I don\u0026rsquo;t think will be that helpful, unless you wanna see google translate fail miserably. Also, I won\u0026rsquo;t get into much detail here because I have a post coming out exactly on this topic, not because I\u0026rsquo;m lazy. It\u0026rsquo;s a two years commitment just like any other master, nothing fancy, lots of statistics. Various modules and courses that can hint you what it means to be a data scientist working on genomics, ranging from regression models and genomic epidemiology to statistics for genomics and bioinformatics applied to big data. Oh yeah, and a never-ending internship that\u0026rsquo;ll make you regret your life\u0026rsquo;s choices.\nR for Data Science Probably one of the most complete resources on the \u0026ldquo;basics\u0026rdquo; of R for data science and by far the most cited resource to start with data science in R that I\u0026rsquo;ve found. If you want a hard copy, help yourself here. And basics\u0026rsquo; s in quotes for a reason and I\u0026rsquo;ll get back to that in a second.\nIt\u0026rsquo;s structured like this: it throws you directly into the visualization/data manipulation/exploratory data analysis world, without knowing anything about programming; and then teaches you about the fundamentals of data wrangling (data import, tidying, strings, and factors), programming (functions, vectors, iterations and so on) and modeling. Now, while I do believe the modeling part should be taught at the very end, I wonder why the core programming is taught later in the book. I was confused at the beginning, especially when manipulating data with dyplr, and I know some stuff about R.\nI\u0026rsquo;m picturing someone approaching this for the very first time going like \u0026ldquo;the heck is this? Seriously, at some point was really hard to follow along even knowing the syntax, I can only imagine the struggle a complete newbie has to go through. That\u0026rsquo;s why I\u0026rsquo;ve placed basics in quotes at the beginning, while some parts are really toddler material, this is actually a bit more complex than other introductions to R that you can find online (IMO). I found out that when we start using flights data frame (classic data frame used in data science for teaching) my brain cells just crumble and die.\nBesides this, I do think it\u0026rsquo;s a stupidly good book for learning data science and imma cherry-pick, from the chapters, what I suck at and work on it. But now that I think of it, I don\u0026rsquo;t think I can use \u0026ldquo;cherry-pick\u0026rdquo; for stuff I\u0026rsquo;m bad at, right? But what if, you do enjoy slamming your face onto your keyboard and google searching every other minute? Is that cherry-picking stuff you suck at a positive behavior? The only way to get better at it? Am I losing my own thread here? I think so, let\u0026rsquo;s move on before it\u0026rsquo;s too late.\nMachine Learning A-Z™: Hands-on Python \u0026amp; R In Data Science I\u0026rsquo;ll be brutally honest with ya, this was utterly unnecessary at this point, but I was so goddam intrigued. Be it because ML is everywhere if you open up Twitter; every other post in data science threads is some random 13 years old that\u0026rsquo;s teaching ML to grownups \u0026lsquo;cause he\u0026rsquo;s already figured out between recesses, what he wants to do with his own life, while I\u0026rsquo;m there, not sure what I\u0026rsquo;ll cook for dinner. Seriously, it\u0026rsquo;s everywhere. Dunno, why I haven\u0026rsquo;t started this before (no wait I know, I think it\u0026rsquo;s called lacking foundations).\nAnyway, now that I pretty much know what\u0026rsquo;s going on in R, I thought this would be the perfect time to dive deep into the subject. I was particularly interested in what\u0026rsquo;s happening under the hood, mostly because R makes super easy running algorithms, thus I just can\u0026rsquo;t go running around and pressing buttons hoping to get something out. Apparently, you gotta understand what you\u0026rsquo;re doing.\nThe course is structured so you can use both R and Python and tries to give you a foundation on the most used algos for machine learning. It\u0026rsquo;s something like \u0026ldquo;here are your toys, now figure how what you like and how to use them\u0026rdquo;. But we\u0026rsquo;ll talk about this a lot in future posts.\nCourses I\u0026rsquo;m taking this year The following are courses I\u0026rsquo;m planning on taking in the next few months. The underlying idea is to mix things up and integrate more field-specific courses with others that are broader and focus more on data science per se. This is what I came up with for now and hopefully, I\u0026rsquo;ll be able to get them done by the end of the year\nSingle Cell RNA-seq Data Analysis with R offered by CSC First one up on the list is, without any shadow of a doubt, scRNA-seq analysis (single-cell RNA sequencing). The course is sponsored by elixir EXCELERATE program and organized by the Finnish company CSC. The course is structured with video lessons that can be found on their youtube channel, slides, and exercise notebooks on GitHub.\nLong story short, the course starts with data quality and preprocessing, normalization, confounding removal, and data integration. Then it focuses on dimensionality reduction, clustering, DGEA (Differential Gene Expression Analysis) and ends with cell-type identification, trajectories/Pseudo-time and spatial transcriptomics. Even though high-dimensional data concepts are being explained throughout the course, better integration is required. Thus, the courses on edX and Coursera are just about that.\nData Visualization \u0026amp; Dashboarding with R on Coursera A bit more of a general-purpose compilation of courses of data visualization. I\u0026rsquo;m skipping quite a bit here \u0026lsquo;cause it repeats a lot of the basics and hopefully, at this point I\u0026rsquo;m decent enough with R. First course is all about importing, tidying data and there\u0026rsquo;s a lot of info on reporting with R Markdown, and that\u0026rsquo;s why I\u0026rsquo;ve chosen this course, mainly.\nSecond course\u0026rsquo;s main topic is ggplot2, of course. Cant\u0026rsquo; go wrong with that guy. I\u0026rsquo;ll quickly browse through this again, but most of the work it\u0026rsquo;s already done in \u0026ldquo;R for Data Science\u0026rdquo;.\nThird course\u0026rsquo;s the previous one but on steroids.\nLastly, the fourth one, is all about publishing visualizations with Shiny and Flexdashboard. We going fancy fam.\nStatistics for Genomic Data Science on Coursera This is a course that I\u0026rsquo;d consider borderline between field-specific biology and multi-purpose statistics. The reason is that, through the course, most of the statistics that are being taught are pretty much all the time applied to genomics and are case-specific. I\u0026rsquo;ll take this hoping that it\u0026rsquo;d cement my knowledge on exploratory data analysis, normalization, preprocessing, clustering and modeling for genomics.\nAlso, in the last module they give you common pipelines for RNA/ChiP-seq, DNA methylation studies, and GWAS. Who am I to refuse this?\nStatistical Inference and Modeling for High-throughput Experiments on edX This definitely will be the hardest one, mostly \u0026lsquo;cause of statistics overload. It\u0026rsquo;ll either bend me or break me, and there\u0026rsquo;s no going back. This is my public statement of commitment. The course focuses on a variety of statistical topics, ranging from multiple testing, error rate controls, false discovery rates to statistical modeling and Bayesian statistics.\nI know, from the very beginning, that this will cause a stupid amount of pain, but what really hurts is that, deep down, I know that I need this. You can\u0026rsquo;t run around the Genomics playground making your own statistics up. You gotta learn this. I gotta learn this. So, off I go, I guess.\nHigh dimensional Data Analysis on edX This is the first integration to the scRNA-seq course from CSC and provided by HarvardX on the edX platform. This course revolves around concepts like mathematical distance, dimensionality reduction, singular value decomposition (SVC), and principal component analysis(PCA).\nOther modules that I find appealing are how to deal with batch effects, heatmaps and clustering, while I\u0026rsquo;m probably dropping the intro to machine learning also provided by them. I have nothing funny to say about this, sorry.\nCase Studies in Functional Genomics on edX I\u0026rsquo;ll probably take this one at the same time as the High Dimensional Data analysis, giving the fact this is focusing especially on the analysis of RNA-seq, DNA methylation and ChiP-seq data. Also, it\u0026rsquo;s gonna give a basic understanding of how reads are being mapped to the reference genome and how to assess the quality of NextGen data.\nBasically, NextGen data will become your best friend.\nHarvardX Biomedical Data Science Open Online Training I\u0026rsquo;m gonna make this quick, there\u0026rsquo;s this GitHub repo that\u0026rsquo;s been made from this book, hard copy here, or maybe is the other way around, and it\u0026rsquo;s divided into three sections: Data Analysis for the Life Sciences, Genomics Data Analysis and Using Python for Research.\nWhile the Python section is a hard pass (even though at some point I gotta learn that sneaky little brat), the others look super interesting even if they\u0026rsquo;re most likely repeating some stuff. As a general integration to the other courses, they seem appropriate.\nSQL for data science on Coursera Well, there\u0026rsquo;s not much to say here, you can\u0026rsquo;t work in any data science-related field without knowing a wee bit of SQL.\nThis is it, lots of stuff, not much time, way less determination. Let\u0026rsquo;s see what the future holds.\nSee ya.\n","date":"2021-03-15T00:00:00Z","image":"https://genstud.netlify.app/p/what-a-year-ahead/zaksheuskaya_hu3d03a01dcc18bc5be0e67db3d8d209a6_1525738_120x120_fill_q75_box_smart1.jpg","permalink":"https://genstud.netlify.app/p/what-a-year-ahead/","title":"What a Year Ahead"},{"content":"All right, let\u0026rsquo;s cut to the chase, creating this Blog with RStudio, blogdown, and Hugo themes after just a week of introduction to R programming for my MSc\u0026rsquo;s program, was hella hard and a pain in the butt. I\u0026rsquo;m not gonna lie.\nMost of the tutorials/how-to that you can find online, be it on video format or another person\u0026rsquo;s blog, are absolutely great; for them and for their settings and probably won\u0026rsquo;t transfer as well as you might think in your specific case. There\u0026rsquo;s always some troubleshooting you gotta do that\u0026rsquo;s not covered anywhere and you might need to come up with something on your own, merging together tips from others. I\u0026rsquo;m not saying it\u0026rsquo;s impossible of course, but I was surprised to see how hard and sometimes no sense (at least for now, I\u0026rsquo;ll probably get better in the future) it is.\nEven with blogdown and Hugo, which should be the no-brainer-JavaScript-HTML-free way of making a blog, was hard af. I\u0026rsquo;d chosen a different theme to start with but had to drop it after not being able to troubleshoot a deploying problem.\nEnough with the rant, let\u0026rsquo;s spread some positive vibes, shall we?\nRelevant Resources  Main Article from Bobby Muljono (towards data science) Setting up GitHub repo Hugo Stack Theme  All the relevant references are here, but you are most likely to do some good ol' googling, to find out everything you need.\nTL;DR you create a new repo on RStudio with blogdown where you choose your theme, create a repo on GitHub that will be integrated with Git, modify your theme (this is the hardest part btw, but you\u0026rsquo;ll get better once you get used to it) and finally, you need a platform that deploys your website and hosts it (in my case I\u0026rsquo;ve used Netlify, that takes your files in your GitHub repo and turns it into what you are seeing now).\nThough the process is quite straightforward, once you get to the personalization of your website, that\u0026rsquo;s when it gets tricky. There should be, and that\u0026rsquo;s not always the case, so keep that in mind, documentation for every Hugo theme you decide to use and general documentation on the architecture of Hugo and the syntax. Basically, documents that explain how Hugo thinks. And, that\u0026rsquo;s as crucial as it gets. Once you\u0026rsquo;ve entered that rabbit hole, all you gotta do is figure out through trial and error what goes where, and when the blog throws a tantrum because that goddamn image is not in the right folder.\nIf you are like me and have absolutely zero coding/computer science background, sometimes the terminology used makes no sense whatsoever and the only way out is to bang your head against a wall multiple times before getting to that \u0026ldquo;oh god, thanks it worked\u0026rdquo; moment. Last tip i can give you is to look at the examples provided , build the server on local host and tweak them as much as you can. That will hint you what you can and cannot do in order to manipulate the layout of the blog.\nThat\u0026rsquo;s it, this was mostly a general test for me, about the blog and the text piece, which was a lot harder than I thought it would. Juicier things are coming about coursers imma about to take and report here, and a general overview of the Msc program I\u0026rsquo;m currently in.\nI know this might sound super easy to professional webdevs and whatnot, but we not that good alright? So, if you were able to get through this\nHere\u0026rsquo;s your reward. My dog being cute  you thic ass boi \nSee ya!\n","date":"2021-02-27T00:00:00Z","image":"https://genstud.netlify.app/p/new-test-for-post/pawel-czerwinski-8uZPynIu-rQ-unsplash_hud7e36f7e20e71be184458283bdae4646_55974_120x120_fill_q75_box_smart1.jpg","permalink":"https://genstud.netlify.app/p/new-test-for-post/","title":"Setting Up The Blog"}]